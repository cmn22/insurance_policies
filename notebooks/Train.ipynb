{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59cd545e",
   "metadata": {},
   "source": [
    "# Model Training Walkthrough (RandomForestRegressor)\n",
    "\n",
    "This notebook trains a **RandomForestRegressor** on the processed datasets created by the *prepare* step,\n",
    "evaluates it, and saves artifacts (model + reports). It mirrors the logic in `src/train.py` but with detailed\n",
    "explanations and inline outputs for readability.\n",
    "\n",
    "**What you'll see:**\n",
    "- Load **params.yaml** (for paths and model hyperparameters)\n",
    "- Load **processed CSVs** (`X_train.csv`, `X_test.csv`, `y_train.csv`, `y_test.csv`)\n",
    "- Train **RandomForestRegressor**\n",
    "- Compute **metrics** (RMSE, R², MSE on train/test)\n",
    "- Save **artifacts** (`models/model.joblib`, reports/metrics.json, and feature_importance.csv)\n",
    "- Visualize **feature importances**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260c3679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# Modeling\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from joblib import dump\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "pd.set_option(\"display.width\", 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7be8e3",
   "metadata": {},
   "source": [
    "## 1. Locate the project root and `params.yaml`\n",
    "\n",
    "We resolve the **project root** as the directory that contains your `params.yaml`.  \n",
    "This makes the notebook location-agnostic (works if you place it under `notebooks/` or elsewhere)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_project_root(start: Path) -> Path:\n",
    "    \"\"\"Walk upward from `start` to find directory containing params.yaml.\"\"\"\n",
    "    cur = start.resolve()\n",
    "    for p in [cur, *cur.parents]:\n",
    "        if (p / \"params.yaml\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Could not locate 'params.yaml' in current or parent directories.\")\n",
    "\n",
    "# Assume the notebook is run from its own directory; adjust as needed.\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT = find_project_root(NOTEBOOK_DIR)\n",
    "PARAMS_PATH = ROOT / \"params.yaml\"\n",
    "\n",
    "ROOT, PARAMS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05157509",
   "metadata": {},
   "source": [
    "## 2. Load parameters\n",
    "\n",
    "We read `params.yaml` to get paths, the target column, split proportions, and (optionally) model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2062e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PARAMS_PATH, \"r\") as f:\n",
    "    params = yaml.safe_load(f)\n",
    "\n",
    "# Show the relevant portions\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a86f8c",
   "metadata": {},
   "source": [
    "## 3. Load processed datasets\n",
    "\n",
    "We expect the following files from the **prepare** stage:\n",
    "\n",
    "- `data/processed/X_train.csv`\n",
    "- `data/processed/X_test.csv`\n",
    "- `data/processed/y_train.csv`\n",
    "- `data/processed/y_test.csv`\n",
    "\n",
    "The helper below reads `y_*` safely whether saved with or without a header row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_processed(root: Path):\n",
    "    processed = root / \"data\" / \"processed\"\n",
    "    X_train = pd.read_csv(processed / \"X_train.csv\")\n",
    "    X_test  = pd.read_csv(processed / \"X_test.csv\")\n",
    "\n",
    "    def read_y(path: Path, n_expected: int) -> pd.Series:\n",
    "        # Try with header (default)\n",
    "        y = pd.read_csv(path).iloc[:, 0]\n",
    "        if len(y) == n_expected:\n",
    "            return y\n",
    "        # Fallback: no header saved\n",
    "        y2 = pd.read_csv(path, header=None).iloc[:, 0]\n",
    "        if len(y2) == n_expected:\n",
    "            return y2\n",
    "        raise ValueError(\n",
    "            f\"Inconsistent length when reading {path}. \"\n",
    "            f\"Got {len(y)} (header) and {len(y2)} (no header). \"\n",
    "            f\"Expected {n_expected}.\"\n",
    "        )\n",
    "\n",
    "    y_train = read_y(processed / \"y_train.csv\", len(X_train))\n",
    "    y_test  = read_y(processed / \"y_test.csv\", len(X_test))\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = read_processed(ROOT)\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"X_train:\", X_train.shape, \"| X_test:\", X_test.shape)\n",
    "print(\"y_train:\", y_train.shape, \"| y_test:\", y_test.shape)\n",
    "\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd8a211",
   "metadata": {},
   "source": [
    "## 4. Train the RandomForest model\n",
    "\n",
    "We use `n_estimators`, `max_depth`, `n_jobs`, and `random_state` from `params.yaml` (if present).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc38b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = params.get(\"model\", {}) or {}\n",
    "\n",
    "def build_rf(cfg: dict) -> RandomForestRegressor:\n",
    "    md = cfg.get(\"max_depth\")\n",
    "    max_depth = None if md in (None, \"None\") else int(md)\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=int(cfg.get(\"n_estimators\", 200)),\n",
    "        max_depth=max_depth,\n",
    "        n_jobs=int(cfg.get(\"n_jobs\", -1)),\n",
    "        random_state=int(cfg.get(\"random_state\", 42)),\n",
    "    )\n",
    "    return rf\n",
    "\n",
    "rf = build_rf(model_cfg)\n",
    "rf.fit(X_train, y_train)\n",
    "rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d32adcf",
   "metadata": {},
   "source": [
    "## 5. Evaluate the model\n",
    "\n",
    "We compute **MSE** and derive **RMSE** (square root of MSE). We also report **R²** for both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e30ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def evaluate(model, X_tr, y_tr, X_te, y_te):\n",
    "    preds_tr = model.predict(X_tr)\n",
    "    preds_te = model.predict(X_te)\n",
    "\n",
    "    mse_tr = mean_squared_error(y_tr, preds_tr)\n",
    "    mse_te = mean_squared_error(y_te, preds_te)\n",
    "    rmse_tr = sqrt(mse_tr)\n",
    "    rmse_te = sqrt(mse_te)\n",
    "    r2_tr = r2_score(y_tr, preds_tr)\n",
    "    r2_te = r2_score(y_te, preds_te)\n",
    "\n",
    "    return {\n",
    "        \"rmse\": float(rmse_tr),\n",
    "        \"rmse_test\": float(rmse_te),\n",
    "        \"r2\": float(r2_tr),\n",
    "        \"r2_test\": float(r2_te),\n",
    "        \"mse\": float(mse_tr),\n",
    "        \"mse_test\": float(mse_te),\n",
    "    }\n",
    "\n",
    "metrics = evaluate(rf, X_train, y_train, X_test, y_test)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0323eb96",
   "metadata": {},
   "source": [
    "## 6. Save artifacts (model + reports)\n",
    "\n",
    "We save:\n",
    "- `models/model.joblib` (the trained estimator)\n",
    "- `models/feature_names.json` (for inference-time alignment)\n",
    "- `reports/metrics.json` (for auditing and CI checks)\n",
    "- `reports/feature_importance.csv` (optional helper for interpretation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b40a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_artifacts(root: Path, model, X_train, metrics: dict):\n",
    "    models_dir = root / \"models\"\n",
    "    reports_dir = root / \"reports\"\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model_path = models_dir / \"model.joblib\"\n",
    "    dump(model, model_path)\n",
    "\n",
    "    feature_names = list(X_train.columns)\n",
    "    (models_dir / \"feature_names.json\").write_text(json.dumps(feature_names, indent=2))\n",
    "    (reports_dir / \"metrics.json\").write_text(json.dumps(metrics, indent=2))\n",
    "\n",
    "    # Optional feature importances\n",
    "    try:\n",
    "        fi = pd.DataFrame({\n",
    "            \"feature\": feature_names,\n",
    "            \"importance\": getattr(model, \"feature_importances_\", [])\n",
    "        }).sort_values(\"importance\", ascending=False)\n",
    "        fi_path = reports_dir / \"feature_importance.csv\"\n",
    "        fi.to_csv(fi_path, index=False)\n",
    "    except Exception:\n",
    "        fi_path = None\n",
    "\n",
    "    return {\n",
    "        \"model\": str(model_path),\n",
    "        \"feature_names\": str(models_dir / \"feature_names.json\"),\n",
    "        \"metrics\": str(reports_dir / \"metrics.json\"),\n",
    "        \"feature_importance\": str(fi_path) if fi_path else None,\n",
    "    }\n",
    "\n",
    "artifacts = save_artifacts(ROOT, rf, X_train, metrics)\n",
    "artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62990a5d",
   "metadata": {},
   "source": [
    "## 7. Visualize feature importances\n",
    "\n",
    "We plot the top features by importance to get a sense of what the RandomForest found informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada3b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only plot if the attribute exists and has values\n",
    "if hasattr(rf, \"feature_importances_\") and len(getattr(rf, \"feature_importances_\", [])):\n",
    "    fi = pd.DataFrame({\n",
    "        \"feature\": list(X_train.columns),\n",
    "        \"importance\": rf.feature_importances_\n",
    "    }).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    top_n = 15 if fi.shape[0] > 15 else fi.shape[0]\n",
    "    top_fi = fi.head(top_n).iloc[::-1]  # reverse for a nicer horizontal plot\n",
    "\n",
    "    plt.figure(figsize=(8, max(4, top_n * 0.4)))\n",
    "    plt.barh(top_fi[\"feature\"], top_fi[\"importance\"])\n",
    "    plt.title(\"Top Feature Importances\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    top_fi\n",
    "else:\n",
    "    print(\"Model has no feature_importances_ attribute; skipping plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de29da",
   "metadata": {},
   "source": [
    "## 8. Recap\n",
    "\n",
    "- We loaded configuration from `params.yaml` (paths + model hyperparameters).\n",
    "- We read the processed train/test splits from `data/processed/`.\n",
    "- We trained a `RandomForestRegressor`, computed metrics (RMSE, R², MSE), and saved artifacts.\n",
    "- We visualized **feature importances** for quick interpretation.\n",
    "\n",
    "This notebook is presentation-friendly and aligns 1:1 with `src/train.py` (sans MLflow)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
